\section{Methods}\label{sec:method}

% Subsection Feature Engineering
\subsection{Feature Engineering}
As there are in total 48 features corresponding to only 10000 data poinst, it has pitfalls
if the ML model is trained onto all features which number of them show weak correlationship to
the labels. Additionally, with the fixed number of training samples, the predictive
power of a ML model starts deteriorating when the number of dimensions, or features, exceeds
a certain dimensionality~\cite{problem-dimension}. Hence, keeping the number of features growing
does mean increasing the accuracy of predictions.

To decide which subset of features I should rely on, the baseline features were referenced,
showed in~\cite{CHIEW2019153}. That baseline feature set were extracted by a feature selection
framework introduced in~\cite{CHIEW2019153}. Along with those 10 baseline features, the correlation
coefficients between all features and label are analyzed by Mutual Information (MI).
MI was chosen since it measures the general dependence between variables, both linear and non-linear
relationship, which is unlike other common approaches such as Spearman or Pearson which only measure
the linear dependence~\cite{MI-score}.\autoref{fig:mi_score} shows MI scores of all 48 features
sorted in the ascending order. Top 15 features which have the highest MI score were selected then were
union-ed to the baseline feature subset to have the final feature sets for training. As a result,
a set of 16 features was introduced as shown in~\autoref{tab:feature_description}. The number of
features, or dimensions, is sufficient for the dataset of 10,000 entries and not too large leading
to the curse of dimensionality.


% Subsection Machine Learning Model
\subsection{Model}
\subsubsection{Logistic Regression}
In this supervised classification task, Logistic Regression (LR) was proposed as a machine learning method since it
is a binary classification and easily interpretable. LR categorizes data points by feature vectors
$\mathbf{x} \in \mathbb{R}^n$ and binary lables $y$, $y \in \{0,1\}$ (encoded to \{Legitimate, Phishing\}) in this case.
The model uses a linear hypothesis $h(\mathbf{x}) = w^T\mathbf{x}$, with some parameter vector
$w \in \mathbb{R}^n$, to predict a binary label $y$~\cite{ml-book}.

The binary logistic loss function was considered
\begin{align}
    L((\mathbf{x},y),h) = log(1+exp(-yh(\mathbf{x})))
\end{align}
where $x$ is feature vectors, $y$ is the true binary label (0 or 1), and $h$ is the linear hypothesis~\cite{ml-book}.
The reason behind this selection is that it is intuitive and already integrated into
\href{https://scikit-learn.org/stable/}{scikit-learn}.
Moreover, it penalizes incorrect predictions more heavily than correct predictions, which is suitable for
the aim of the task of detecting phishing websites.

\subsubsection{Random Forest}
As shown in~\cite{CHIEW2019153,SAHINGOZ2019345}, Random Forest (RF) outperforms other
classifier in phishing detection. Additionally, unlike LR, RF does not rely on the
linear relationship so that it is able to handle non-linear, more complex, relationships
between features. Those were two main logically reasons leaded me to choose RF as
a second machine learning model for this phishing dectection task. The loss function for
RF in classification is the impurity of individual decision region for labels~\cite{ml-book}.

Regarding the splitting criteria, loosely understood as a loss function for decision trees,
Gini impurity was selected due to its fast computation and good performance on balanced
data set. Gini impurity measures the heterogeneity of a node, quantifying how often
a randomly chosen element from the set would be incorrectly classified if it was labeled
according to the distribution of labels in that node. Mathematically, Gini impurity is
expressed:

\begin{align}
    I_G(p) = 1 - \sum_{i=1}^{K}p_i^2
\end{align}

where $K$ is the set of classes, $p_i$ is the probability of choosing an item with label
$i$~\cite{gini-formula}. The goal during training is to minimize the Gini impurity of
the resulting child nodes, creating more homogeneous groups.

\subsubsection{Model Evaluation}

To evaluate the models, \emph{Precision} score was measured as the main benchmark during hyperparameter-tuning.
Additionally, in a phishing dectection context, it is also more important to dectect all
phishing pages, even if some legitimate pages are misclassified, so the \emph{Recall} metric was also
considered during hyperparameter-tuning. Hence, \emph{F1-Score } was also used to find balance
between \emph{Precision} and \emph{Recall} metrics.

% Subsection Training and Validation Dataset
\subsection{Training and Validation Set}
The dataset itself is balanced between two classes, 5000 data poitns for each class, so class balancing step
is no needed. Following the ``thumbs-up'' rule, the data set was divided into three sets, following the ratio 70/15/15,
into three subsets: training, validation, and testing. As a result, the training, validation, and testing subset
has 7000, 1500, and 1500 data points respectively. This ratio of splitting is reasonable as it is widely used and accepted
in the machine learning community. Moreover, this ``hold-out'' technique generates a sufficiently large size of
training and validation data. Importantly, the data set was shuffled to avoid biased sampling. The training
set was used to train the learning model, then the validation set was used for fine-tuning. If the current parameter
and configuration of the hypothesis model has a good performance, final testing and benchmark are operated on the
testing set.